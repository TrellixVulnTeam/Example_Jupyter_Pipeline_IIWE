{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/AzPTravel_PPM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Source File Consolidation\n",
    "\n",
    "#### This script fetches the current data collection submission files, consolidates them in to a combined csv file, and outputs that to the network.\n",
    "- All source files stored in the path are read, summarized, and then consolidated into a single dataframe named 'df'.\n",
    "- This process assumes that the multiple sources are in no way duplicates of each other.\n",
    "- df is then output in the data collection subfolder of the \"Production\" folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Variables\n",
    "- These are overwritten if inherited from run_control.ipynb.\n",
    "- Feel Free to reset them for a manual run if you like\n",
    "- Do not save without percode = \"-f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "commit_message = \"Development and testing.\"\n",
    "# Give a brief reason for the run.\n",
    "\n",
    "run_control = 1\n",
    "#run_type = 0 - Lite run with no reporting, not recommended.\n",
    "#run_type = 1 - Lite run with normal reporting, default setting.\n",
    "#run_type = 2 - Heavy run with full reporting, available for audits and troubleshooting.\n",
    "#run_type = 5 - A default setting. Indicates the script is being run by an outside process without an inherited value\n",
    "\n",
    "percode = \"2021.Q1\"\n",
    "# Data Collection Code, this controls file paths and output names\n",
    "# \"-f\" is the value indicating a bad inheritance from run with arg\n",
    "\n",
    "s_format = \"p\"\n",
    "# denotes the source data format x == Excel; j == json, p == parquet\n",
    "\n",
    "#----------\n",
    "# do not edit - this either inherits the full instance timestamp from the papermill book or captures the run time of this script.\n",
    "from datetime import datetime\n",
    "inst_datetime = datetime.now().strftime(\"%m%d%Y%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Packages used\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import glob\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nicexcel as nl\n",
    "import xlsxwriter\n",
    "\n",
    "global df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Variables, these govern logic, do not edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "default_dc = \"20XX.QX\"\n",
    "default_rc = 0 #extra lite mode\n",
    "dummy_perc = \"33Q3\" # bad inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Script determining run context ie, manual, run_control.ipynb, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "if run_control == 5:\n",
    "    run_control = default_rc \n",
    "else:\n",
    "    run_control = run_control\n",
    "\n",
    "try:\n",
    "    if sys.argv[1] == \"-f\":\n",
    "        percode = percode\n",
    "    else:\n",
    "        percode = sys.argv[1]\n",
    "\n",
    "except IndexError:\n",
    "    percode = default_dc\n",
    "except NameError:\n",
    "    percode = default_dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### style settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Make paths for the source folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rt_path = f'//hecate/Insurance_US/Product Development/Product Management/Global PPM/Reporting/Data Collection/Production/{str(percode)}'\n",
    "ls_path = os.path.join( rt_path, 'live_sources')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Get a list of only source files in the path that start with \"us_dat\".\n",
    "#### Logic determines the source file types.\n",
    "\n",
    "#### User instructions:\n",
    "- Make sure that you have 1 file per source in this folder.\n",
    "    -  For instance, do not have two files for Portugal. If there is an update, archive the old one.\n",
    "- Do not overwrite files in the archive.\n",
    "    - Rename newly archived files, no strict convention, we keep track of these by the modified date.\n",
    "- It is ok to have multiple sources in one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us_dat_nz+_05142021114427.parquet',\n",
       " 'us_dat_us_05122021094008.parquet',\n",
       " 'us_survey_us_05122021094728.parquet',\n",
       " 'us_vcomments_nz+_05142021114428.json',\n",
       " 'us_vcomments_nz+_05142021114428.parquet',\n",
       " 'us_vcomments_us_05122021094708.json',\n",
       " 'us_vcomments_us_05122021094708.parquet',\n",
       " 'us_vdf_nz+_05142021114428.json',\n",
       " 'us_vdf_us_05122021094708.json']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(ls_path)\n",
    "files = [files.lower() for files in files]\n",
    "\n",
    "files_sour = [f for f in files if f[-5:]  == '.json' or  f[-8:] == '.parquet' and f[:7] != 'us_orig']\n",
    "\n",
    "files_sour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NZ+', 'US']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_jfiles = glob.glob(os.path.join(ls_path, 'us_vcomments_*.json'))\n",
    "list_pfiles = glob.glob(os.path.join(ls_path, 'us_vcomments_*.parquet'))\n",
    "\n",
    "for idx, i in enumerate(list_jfiles):\n",
    "    list_jfiles[idx] = list_jfiles[idx][-23:-19]\n",
    "    list_jfiles[idx] = list_jfiles[idx].replace('_','')\n",
    "\n",
    "for idx, i in enumerate(list_pfiles):\n",
    "    list_pfiles[idx] = list_pfiles[idx][-26:-22]\n",
    "    list_pfiles[idx] = list_pfiles[idx].replace('_','')\n",
    "\n",
    "list_files = []\n",
    "    \n",
    "for i in list_jfiles : list_files.append(i)    \n",
    "for i in list_pfiles : list_files.append(i)\n",
    "\n",
    "\n",
    "BU_set = set(list_files)\n",
    "BU_list = list(BU_set)\n",
    "\n",
    "BU_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us_vcomments_nz+_05142021114428.parquet',\n",
       " 'us_vcomments_us_05122021094708.parquet']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_cmnts = [i for i in files_sour if \"vcomments\" in i  and \"parq\" in i ]\n",
    "\n",
    "files_cmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "latest_valcomments = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in files_cmnts:\n",
    "    list_files_a = glob.glob(os.path.join(ls_path, f'us_vcomments_{BU_list[idx]}*.parquet'))\n",
    "    recent_vers_a = max(list_files_a, key=os.path.getctime)\n",
    "    latest_valcomments.append([f'{i}',pd.read_parquet(recent_vers_a, engine = \"pyarrow\")])\n",
    "\n",
    "    list_files_b = glob.glob(os.path.join(ls_path, f'us_vdf_{BU_list[idx]}*.json'))\n",
    "    recent_vers_b = max(list_files_b, key=os.path.getctime)\n",
    "    with open(recent_vers_b, 'r') as fp:\n",
    "        data_dict = json.load(fp)\n",
    "    latest_vdfdata_sets = { key: pd.DataFrame(data_dict[key]) for key in data_dict  }\n",
    "\n",
    "valcomm_dict = {}\n",
    "valdf_dict = {}\n",
    "valcomdf = pd.DataFrame(latest_valcomments[0][1])\n",
    "\n",
    "for idx2, row in valcomdf.iterrows():\n",
    "\n",
    "    valname = row[\"Validation Rule\"][0:25]\n",
    "    sheetname = valname + '-' + str(idx2)\n",
    "    valcomm_dict[\"{0}\".format(sheetname)] = pd.DataFrame(row).reindex()\n",
    "    valdf_dict[sheetname] = data_dict[str(idx2)]\n",
    "\n",
    "#writer = pd.ExcelWriter(os.path.join(os.path.dirname(__file__), f'Output/Validations Review _{BU[idx]}.xlsx'),\n",
    "                        #engine='xlsxwriter')\n",
    "for i, j  in zip(valdf_dict,valcomm_dict ):\n",
    "    valdf_dict[i] = pd.DataFrame(valdf_dict[i])\n",
    "    valdf_dict[i].index.name = 'Row Number'\n",
    "    #valcomm_dict[j].to_excel(writer, sheet_name=i,header=False, startrow=0, startcol=0)\n",
    "    #valdf_dict[i].to_excel(writer, sheet_name= i , startrow=10, startcol=0)\n",
    "\n",
    "latest_valcomments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "datafiles = [  \"us_vcomment\", \"us_vdf\" ]\n",
    "\n",
    "latest_valcomments = []\n",
    "def load_dfs(BU):\n",
    "\n",
    "    for idx, i in enumerate(BU):\n",
    "\n",
    "        for i in files_sour:\n",
    "            list_files_a = glob.glob(os.path.join(os.path.dirname(__file__), f'Output/us_vcomments_{BU[idx]}*.json'))\n",
    "            recent_vers_a = max(list_files_a, key=os.path.getctime)\n",
    "            latest_valcomments.append([f'{i}',pd.read_json(recent_vers_a, orient=\"table\")])\n",
    "\n",
    "            list_files_b = glob.glob(os.path.join(os.path.dirname(__file__), f'Output/us_vdf_{BU[idx]}*.json'))\n",
    "            recent_vers_b = max(list_files_b, key=os.path.getctime)\n",
    "            with open(recent_vers_b, 'r') as fp:\n",
    "                data_dict = json.load(fp)\n",
    "            latest_vdfdata_sets = { key: pd.DataFrame(data_dict[key]) for key in data_dict  }\n",
    "\n",
    "        valcomm_dict = {}\n",
    "        valdf_dict = {}\n",
    "        valcomdf = pd.DataFrame(latest_valcomments[0][1])\n",
    "\n",
    "        for idx2, row in valcomdf.iterrows():\n",
    "\n",
    "            valname = row[\"Validation Rule\"][0:25]\n",
    "            sheetname = valname + '-' + str(idx2)\n",
    "            valcomm_dict[\"{0}\".format(sheetname)] = pd.DataFrame(row).reindex()\n",
    "            valdf_dict[sheetname] = data_dict[str(idx2)]\n",
    "\n",
    "        #writer = pd.ExcelWriter(os.path.join(os.path.dirname(__file__), f'Output/Validations Review _{BU[idx]}.xlsx'),\n",
    "                                #engine='xlsxwriter')\n",
    "        for i, j  in zip(valdf_dict,valcomm_dict ):\n",
    "            valdf_dict[i] = pd.DataFrame(valdf_dict[i])\n",
    "            valdf_dict[i].index.name = 'Row Number'\n",
    "            valcomm_dict[j].to_excel(writer, sheet_name=i,header=False, startrow=0, startcol=0)\n",
    "            valdf_dict[i].to_excel(writer, sheet_name= i , startrow=10, startcol=0)\n",
    "\n",
    "            print(i)\n",
    "\n",
    "      #  writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create a list 'pathfiles' that has every source file with the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pathfiles = []\n",
    "\n",
    "for f in files_sour:\n",
    "    makepathsfiles = os.path.join(str(ls_path), str(f))\n",
    "    pathfiles.append(makepathsfiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Check\n",
    "\n",
    "- create file information Data.Frame \"file_info_df\" with name, size, and modified date of each source file\n",
    "- import previous source file list for comparison\n",
    "- print all new files, store as a list \"newfiles\"\n",
    "- print all dropped files, store as a list \"dropped_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New new or changed data files found in live sources folder.\n"
     ]
    }
   ],
   "source": [
    "#print(\"Current Source Files\")\n",
    "cols = [\"Filename\", \"Size\", \"Last Modified\" ]\n",
    "\n",
    "file_info_df = pd.DataFrame(columns = cols)\n",
    "for f, p in zip(files_sour, pathfiles):\n",
    "    name = f\n",
    "    size = os.path.getsize(p) # size in bytes\n",
    "    moddate = os.path.getctime(p)# time of last metadata change;\n",
    "    moddate =  datetime.fromtimestamp(moddate).strftime('%Y-%m-%d %H:%M:%S')# format change;\n",
    "    new_row = pd.DataFrame([[name, size, moddate]], columns = cols)\n",
    "    file_info_df = file_info_df.append(new_row, ignore_index=True)\n",
    "\n",
    "#print(file_info_df)\n",
    "\n",
    "file_list = os.path.join( rt_path, 'Current_Source_Files.csv')\n",
    "prev_files = pd.read_csv(file_list)\n",
    "\n",
    "\n",
    "fl_comp = file_info_df[\"Filename\"] + file_info_df[\"Size\"].astype(str) + file_info_df[\"Last Modified\"].astype(str)\n",
    "\n",
    "prev_comp = prev_files[\"Filename\"] + prev_files[\"Size\"].astype(str) + prev_files[\"Last Modified\"].astype(str)\n",
    "\n",
    "newfiles = []\n",
    "\n",
    "for idx, r in enumerate(fl_comp):\n",
    "    \n",
    "    if r in prev_comp.values:\n",
    "    \n",
    "        pass\n",
    "     \n",
    "    else:\n",
    "        if file_info_df['Filename'][idx] in prev_files[\"Filename\"].values:\n",
    "            print(\"\")\n",
    "            print(f\"Changed data file found: {file_info_df['Filename'][idx]}, last update {file_info_df['Last Modified'][idx]}.\")\n",
    "            newfiles.append(pathfiles[idx])\n",
    "        else:\n",
    "            print(\"\")\n",
    "            print(f\"New data file found: {file_info_df['Filename'][idx]}, last update {file_info_df['Last Modified'][idx]}.\")\n",
    "            newfiles.append(pathfiles[idx])\n",
    "\n",
    "if len(newfiles) == 0:\n",
    "    \n",
    "    print(\"New new or changed data files found in live sources folder.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    pass\n",
    "\n",
    "dropped_files = []\n",
    "\n",
    "for r in prev_files[\"Filename\"]:\n",
    "    \n",
    "    if r in file_info_df[\"Filename\"].values:\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(f\"File {r} has been archived or removed from live sources folder.\")\n",
    "        dropped_files.append(os.path.join( ls_path, r))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "file_info_df.to_csv(file_list, index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Unit Check\n",
    "\n",
    "- import existing Business Unit and raw file list\n",
    "- enusure there is a 1 to 1 relationship\n",
    "- throw error if not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Business Unit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2890\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2891\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2892\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Business Unit'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6fd80ea625ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"pyarrow\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mbus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Business Unit\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbus\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbu_x_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbu_x_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Business Unit\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"File\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2891\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2893\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2895\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Business Unit'"
     ]
    }
   ],
   "source": [
    "cols = [ \"Business Unit\", \"File\"]\n",
    "\n",
    "bu_x_file = pd.DataFrame(columns = cols ) \n",
    "\n",
    "for p, f  in zip(pathfiles,files_sour):\n",
    "\n",
    "    if s_format == \"x\":\n",
    "\n",
    "        try:\n",
    "            data = pd.read_excel(p, sheet_name = 'Portfolio_Monitoring', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "        except:\n",
    "            data = pd.read_excel(p, sheet_name = 'Ptf_Monitoring_GROSS_Reins', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "\n",
    "    elif s_format == \"j\":\n",
    "\n",
    "        data = pd.read_json(p, orient=\"table\")\n",
    "\n",
    "    elif s_format == \"p\":\n",
    "\n",
    "        data = pd.read_parquet(p, engine = \"pyarrow\")\n",
    "\n",
    "    bus = data[\"Business Unit\"].unique()\n",
    "    \n",
    "    for i in bus: bu_x_file = bu_x_file.append({\"Business Unit\": i,\"File\": f} , ignore_index=True)\n",
    "\n",
    "        \n",
    "#This is a list of BU's that are included in the raw data files, and the file name(s) they are contained in.\n",
    "bu_filelist = bu_x_file.sort_values(by='Business Unit').style.hide_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bucount =  bu_x_file.groupby(\"Business Unit\").count().sort_values(by='File', ascending=False)\n",
    "\n",
    "filecount =  bu_x_file.groupby(\"File\").count().sort_values(by='File')\n",
    "\n",
    "err_check = 1\n",
    "\n",
    "if len(bucount[bucount[\"File\"] > 4]) > 0: # change this back to 1\n",
    "    warnings.warn(\"\\n \\nThere is a Business Unit with data in multiple source files.\\nReview the file counts and filenames to resolve the issue.\\nThis is a critical control.\")\n",
    "    err_check = 0\n",
    "else:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    1 / err_check\n",
    "\n",
    "except ZeroDivisionError:\n",
    "    print(\"Execution halted due to multiple source file per one or more Business Units.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"This is a count of how many BUs appear in each file. It is fine to submit multiple BUs in one source.\")\n",
    "filecount\n",
    "\n",
    "\n",
    "print(\"\\nThis is a count of how many files each BU appears in. In all cases the count values should be one (1). The process as desgined, should have a single submission and source for each BU.\")\n",
    "bucount\n",
    "\n",
    "print(\"This is a list of BU's that are included in the raw data files, and the file name(s) they are contained in.\")   \n",
    "\n",
    "bu_filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Read each json file into temp DataFrame \"data\".\n",
    "- Prep actions - Strip (trim) leading and trailing spaces values in string columns, remove rows without business units.\n",
    "- Append each json output to into 1 DataFrame \"df\".\n",
    "- Process Cleanup, the indices restart with each append, reset and drop the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "#empty dict to append each file's DataFrame to\n",
    "\n",
    "for f in pathfiles:\n",
    "\n",
    "    if s_format == \"x\":\n",
    "\n",
    "        try:\n",
    "            data = pd.read_excel(f, sheet_name = 'Portfolio_Monitoring', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "        except:\n",
    "            data = pd.read_excel(f, sheet_name = 'Ptf_Monitoring_GROSS_Reins', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "\n",
    "    elif s_format == \"j\":\n",
    "\n",
    "        data = pd.read_json(f, orient=\"table\")\n",
    "\n",
    "    elif s_format == \"p\":\n",
    "\n",
    "        data = pd.read_parquet(f, engine = \"pyarrow\")\n",
    "\n",
    "    df_dict.update({f: data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw file Summaries\n",
    "\n",
    "#### First 5 rows of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - First 5 Samples:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.head())\n",
    "\n",
    "else:\n",
    "    print(\"Default Report 2 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statisitics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - Statistical summary:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.describe())\n",
    "else:\n",
    "    print(\"Default Report 3 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Value Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - Null values in the dataset:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.isnull().sum())\n",
    "else:\n",
    "    print(\"Default Report 4 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datatypes by field for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - Datatypes:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.dtypes)\n",
    "else:\n",
    "    print(\"Default Report 5 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the datafile name as a column in each source DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, f in zip(pathfiles, files_sour):\n",
    "    df_dict[p].insert(0, \"Submission File\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### append into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for k, v in df_dict.items(): df = df.append(v)\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Temporary csv Output\n",
    "- Create a direct output of the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prepfile = f'{str(percode)}prep.csv'\n",
    "prepparq = f'{str(percode)}prep.parquet'\n",
    "prephistfile = f'{str(percode)}_{inst_datetime}prep.csv'\n",
    "\n",
    "try:\n",
    "    os.rename(os.path.join(str(rt_path), prepfile), os.path.join(str(rt_path), \"logs/raw_data_file_history/\" , prephistfile))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "except FileExistsError:\n",
    "    os.remove(os.path.join(str(rt_path), \"logs/raw_data_file_history/\" , prephistfile))\n",
    "    os.rename(os.path.join(str(rt_path), prepfile), os.path.join(str(rt_path), \"logs/raw_data_file_history/\" , prephistfile))\n",
    "\n",
    "df.to_csv(os.path.join(str(rt_path), prepfile), ',', index=False , encoding='utf-8-sig')\n",
    "df.to_parquet(os.path.join(str(rt_path), prepparq),engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is considered \"pre-pipleine\" data prep. The next step is to read the resulting csv into the acutal data pipeline.\n",
    "\n",
    "# If you are not sure what is being loaded, this step provides a safe way to create a reviewable output file while keeping the production data files in tact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = (df.pivot_table(index=( \"Country\", \"Submission File\"), columns=(\"Reporting Date From\",\"Reporting Date To\" ), values=\"Earned Revenues net of Taxes\").fillna(0).astype(int))\n",
    "\n",
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
