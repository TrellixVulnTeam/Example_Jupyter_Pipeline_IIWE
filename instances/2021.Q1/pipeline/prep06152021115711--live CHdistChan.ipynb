{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08692d05",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [10]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859f75f",
   "metadata": {
    "papermill": {
     "duration": 0.286793,
     "end_time": "2021-06-15T15:57:27.084206",
     "exception": false,
     "start_time": "2021-06-15T15:57:26.797413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"../images/AzPTravel_PPM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa499ce",
   "metadata": {
    "papermill": {
     "duration": 0.283329,
     "end_time": "2021-06-15T15:57:27.635826",
     "exception": false,
     "start_time": "2021-06-15T15:57:27.352497",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Source File Consolidation\n",
    "\n",
    "#### This script fetches the current data collection submission files, consolidates them in to a combined csv file, and outputs that to the network.\n",
    "- All source files stored in the path are read, summarized, and then consolidated into a single dataframe named 'df'.\n",
    "- This process assumes that the multiple sources are in no way duplicates of each other.\n",
    "- df is then output in the data collection subfolder of the \"Production\" folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46384a1",
   "metadata": {
    "papermill": {
     "duration": 0.227701,
     "end_time": "2021-06-15T15:57:28.100694",
     "exception": false,
     "start_time": "2021-06-15T15:57:27.872993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### User Variables\n",
    "- These are overwritten if inherited from run_control.ipynb.\n",
    "- Feel Free to reset them for a manual run if you like\n",
    "- Do not save without percode = \"-f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b477ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:28.675092Z",
     "iopub.status.busy": "2021-06-15T15:57:28.652378Z",
     "iopub.status.idle": "2021-06-15T15:57:28.679098Z",
     "shell.execute_reply": "2021-06-15T15:57:28.682096Z"
    },
    "papermill": {
     "duration": 0.309829,
     "end_time": "2021-06-15T15:57:28.682096",
     "exception": false,
     "start_time": "2021-06-15T15:57:28.372267",
     "status": "completed"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "commit_message = \"Development and testing.\"\n",
    "# Give a brief reason for the run.\n",
    "\n",
    "run_control = 1\n",
    "#run_type = 0 - Lite run with no reporting, not recommended.\n",
    "#run_type = 1 - Lite run with normal reporting, default setting.\n",
    "#run_type = 2 - Heavy run with full reporting, available for audits and troubleshooting.\n",
    "#run_type = 5 - A default setting. Indicates the script is being run by an outside process without an inherited value\n",
    "\n",
    "percode = \"2021.Q1\"\n",
    "# Data Collection Code, this controls file paths and output names\n",
    "# \"-f\" is the value indicating a bad inheritance from run with arg\n",
    "\n",
    "s_format = \"p\"\n",
    "# denotes the source data format x == Excel; j == json, p == parquet\n",
    "\n",
    "#----------\n",
    "# do not edit - this either inherits the full instance timestamp from the papermill book or captures the run time of this script.\n",
    "from datetime import datetime\n",
    "inst_datetime = datetime.now().strftime(\"%m%d%Y%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb67d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:29.269113Z",
     "iopub.status.busy": "2021-06-15T15:57:29.253462Z",
     "iopub.status.idle": "2021-06-15T15:57:29.269113Z",
     "shell.execute_reply": "2021-06-15T15:57:29.269113Z"
    },
    "papermill": {
     "duration": 0.329418,
     "end_time": "2021-06-15T15:57:29.269113",
     "exception": false,
     "start_time": "2021-06-15T15:57:28.939695",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_control = 1\n",
    "percode = \"2021.Q1\"\n",
    "commit_message = \"Live run, add CH dist Channels\"\n",
    "inst_datetime = \"06152021115711\"\n",
    "source_type = \"p\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30bd32",
   "metadata": {
    "papermill": {
     "duration": 0.300331,
     "end_time": "2021-06-15T15:57:29.933767",
     "exception": false,
     "start_time": "2021-06-15T15:57:29.633436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Notebook display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513b38ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:30.605772Z",
     "iopub.status.busy": "2021-06-15T15:57:30.604773Z",
     "iopub.status.idle": "2021-06-15T15:57:30.610786Z",
     "shell.execute_reply": "2021-06-15T15:57:30.611774Z"
    },
    "papermill": {
     "duration": 0.34153,
     "end_time": "2021-06-15T15:57:30.611774",
     "exception": false,
     "start_time": "2021-06-15T15:57:30.270244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e695cb7",
   "metadata": {
    "papermill": {
     "duration": 0.363489,
     "end_time": "2021-06-15T15:57:31.246047",
     "exception": false,
     "start_time": "2021-06-15T15:57:30.882558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d29c3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:32.008375Z",
     "iopub.status.busy": "2021-06-15T15:57:32.008375Z",
     "iopub.status.idle": "2021-06-15T15:57:39.069733Z",
     "shell.execute_reply": "2021-06-15T15:57:39.069733Z"
    },
    "papermill": {
     "duration": 7.405922,
     "end_time": "2021-06-15T15:57:39.069733",
     "exception": false,
     "start_time": "2021-06-15T15:57:31.663811",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Packages used\n",
    "\n",
    "import os # System commands\n",
    "import sys # System commands\n",
    "\n",
    "from datetime import datetime # datetime options\n",
    "import warnings # custom warnigns options\n",
    "\n",
    "import getpass # parquet file read/write\n",
    "import json # json file read/write\n",
    "\n",
    "import matplotlib.pyplot as plt #Plots and Graphs\n",
    "import openpyxl # Excel operations\n",
    "import numpy as np\n",
    "import pandas as pd #DataFrame and math\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f667bcd9",
   "metadata": {
    "papermill": {
     "duration": 0.30546,
     "end_time": "2021-06-15T15:57:39.644600",
     "exception": false,
     "start_time": "2021-06-15T15:57:39.339140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Default Variables, these govern logic, do not edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1cc448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:40.271494Z",
     "iopub.status.busy": "2021-06-15T15:57:40.271494Z",
     "iopub.status.idle": "2021-06-15T15:57:40.287132Z",
     "shell.execute_reply": "2021-06-15T15:57:40.309291Z"
    },
    "papermill": {
     "duration": 0.372717,
     "end_time": "2021-06-15T15:57:40.309291",
     "exception": false,
     "start_time": "2021-06-15T15:57:39.936574",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_dc = \"20XX.QX\"\n",
    "default_rc = 0 #extra lite mode\n",
    "dummy_perc = \"33Q3\" # bad inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f389f2",
   "metadata": {
    "papermill": {
     "duration": 0.288508,
     "end_time": "2021-06-15T15:57:40.929716",
     "exception": false,
     "start_time": "2021-06-15T15:57:40.641208",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Script determining run context ie, manual, run_control.ipynb, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6075cbe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:41.796103Z",
     "iopub.status.busy": "2021-06-15T15:57:41.796103Z",
     "iopub.status.idle": "2021-06-15T15:57:41.811720Z",
     "shell.execute_reply": "2021-06-15T15:57:41.811720Z"
    },
    "papermill": {
     "duration": 0.4794,
     "end_time": "2021-06-15T15:57:41.811720",
     "exception": false,
     "start_time": "2021-06-15T15:57:41.332320",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "if run_control == 5:\n",
    "    run_control = default_rc \n",
    "else:\n",
    "    run_control = run_control\n",
    "\n",
    "try:\n",
    "    if sys.argv[1] == \"-f\":\n",
    "        percode = percode\n",
    "    else:\n",
    "        percode = sys.argv[1]\n",
    "\n",
    "except IndexError:\n",
    "    percode = default_dc\n",
    "except NameError:\n",
    "    percode = default_dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b68a12",
   "metadata": {
    "papermill": {
     "duration": 0.352901,
     "end_time": "2021-06-15T15:57:42.487654",
     "exception": false,
     "start_time": "2021-06-15T15:57:42.134753",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Make paths for the source folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086bd6a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:43.059208Z",
     "iopub.status.busy": "2021-06-15T15:57:43.059208Z",
     "iopub.status.idle": "2021-06-15T15:57:43.074876Z",
     "shell.execute_reply": "2021-06-15T15:57:43.074876Z"
    },
    "papermill": {
     "duration": 0.300669,
     "end_time": "2021-06-15T15:57:43.074876",
     "exception": false,
     "start_time": "2021-06-15T15:57:42.774207",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rt_path = f'//hecate/Insurance_US/Product Development/Product Management/Global PPM/Reporting/Data Collection/Production/{str(percode)}'\n",
    "ls_path = os.path.join( rt_path, 'live_sources')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e712feb",
   "metadata": {
    "papermill": {
     "duration": 0.271185,
     "end_time": "2021-06-15T15:57:43.673877",
     "exception": false,
     "start_time": "2021-06-15T15:57:43.402692",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Get a list of only source files in the path that start with \"us_dat\".\n",
    "#### Logic determines the source file types.\n",
    "\n",
    "#### User instructions:\n",
    "- Make sure that you have 1 file per source in this folder.\n",
    "    -  For instance, do not have two files for Portugal. If there is an update, archive the old one.\n",
    "- Do not overwrite files in the archive.\n",
    "    - Rename newly archived files, no strict convention, we keep track of these by the modified date.\n",
    "- It is ok to have multiple sources in one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4d22d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:44.362613Z",
     "iopub.status.busy": "2021-06-15T15:57:44.362613Z",
     "iopub.status.idle": "2021-06-15T15:57:44.436290Z",
     "shell.execute_reply": "2021-06-15T15:57:44.440286Z"
    },
    "papermill": {
     "duration": 0.452939,
     "end_time": "2021-06-15T15:57:44.440286",
     "exception": false,
     "start_time": "2021-06-15T15:57:43.987347",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = os.listdir(ls_path)\n",
    "files = [files.lower() for files in files]\n",
    "\n",
    "if s_format == \"x\":\n",
    "    files_sour = [f for f in files if f[-3:] in ('lsx' , 'lsm' ,'xls')]\n",
    "\n",
    "elif s_format == \"j\":\n",
    "    files_sour = [f for f in files if f[:6] == 'us_dat'  and  f[-5:]  == '.json']\n",
    "\n",
    "elif s_format == \"p\":\n",
    "    files_sour = [f for f in files if f[:6] == 'us_dat' and  f[-8:]  == '.parquet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4dd9d",
   "metadata": {
    "papermill": {
     "duration": 0.347762,
     "end_time": "2021-06-15T15:57:45.143057",
     "exception": false,
     "start_time": "2021-06-15T15:57:44.795295",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Create a list 'pathfiles' that has every source file with the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21297bcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:45.806606Z",
     "iopub.status.busy": "2021-06-15T15:57:45.803603Z",
     "iopub.status.idle": "2021-06-15T15:57:45.812602Z",
     "shell.execute_reply": "2021-06-15T15:57:45.814606Z"
    },
    "papermill": {
     "duration": 0.291869,
     "end_time": "2021-06-15T15:57:45.815608",
     "exception": false,
     "start_time": "2021-06-15T15:57:45.523739",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pathfiles = []\n",
    "\n",
    "for f in files_sour:\n",
    "    makepathsfiles = os.path.join(str(ls_path), str(f))\n",
    "    pathfiles.append(makepathsfiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a8907c",
   "metadata": {
    "papermill": {
     "duration": 0.278568,
     "end_time": "2021-06-15T15:57:46.328954",
     "exception": false,
     "start_time": "2021-06-15T15:57:46.050386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### File Check\n",
    "\n",
    "- create file information Data.Frame \"file_info_df\" with name, size, and modified date of each source file\n",
    "- import previous source file list for comparison\n",
    "- print all new files, store as a list \"newfiles\"\n",
    "- print all dropped files, store as a list \"dropped_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5f9ca",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "946e10d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T15:57:46.985696Z",
     "iopub.status.busy": "2021-06-15T15:57:46.983695Z",
     "iopub.status.idle": "2021-06-15T15:57:47.018427Z",
     "shell.execute_reply": "2021-06-15T15:57:47.018427Z"
    },
    "papermill": {
     "duration": 0.331216,
     "end_time": "2021-06-15T15:57:47.018427",
     "exception": true,
     "start_time": "2021-06-15T15:57:46.687211",
     "status": "failed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-4a09567a16c2>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-4a09567a16c2>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    prev_files = pd.read_csv(file_list  encoding='utf-8', engine='python')\u001b[0m\n\u001b[1;37m                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#print(\"Current Source Files\")\n",
    "cols = [\"Filename\", \"Size\", \"Last Modified\" ]\n",
    "\n",
    "file_info_df = pd.DataFrame(columns = cols)\n",
    "for f, p in zip(files_sour, pathfiles):\n",
    "    name = f\n",
    "    size = os.path.getsize(p) # size in bytes\n",
    "    moddate = os.path.getctime(p)# time of last metadata change;\n",
    "    moddate =  datetime.fromtimestamp(moddate).strftime('%Y-%m-%d %H:%M:%S')# format change;\n",
    "    new_row = pd.DataFrame([[name, size, moddate]], columns = cols)\n",
    "    file_info_df = file_info_df.append(new_row, ignore_index=True)\n",
    "\n",
    "#print(file_info_df)\n",
    "\n",
    "file_list = os.path.join( rt_path, 'Current_Source_Files.csv')\n",
    "prev_files = pd.read_csv(file_list  encoding='utf-8', engine='python')\n",
    "\n",
    "\n",
    "fl_comp = file_info_df[\"Filename\"] + file_info_df[\"Size\"].astype(str) + file_info_df[\"Last Modified\"].astype(str)\n",
    "\n",
    "prev_comp = prev_files[\"Filename\"] + prev_files[\"Size\"].astype(str) + prev_files[\"Last Modified\"].astype(str)\n",
    "\n",
    "newfiles = []\n",
    "\n",
    "for idx, r in enumerate(fl_comp):\n",
    "    \n",
    "    if r in prev_comp.values:\n",
    "    \n",
    "        pass\n",
    "     \n",
    "    else:\n",
    "        if file_info_df['Filename'][idx] in prev_files[\"Filename\"].values:\n",
    "            print(\"\")\n",
    "            print(f\"Changed data file found: {file_info_df['Filename'][idx]}, last update {file_info_df['Last Modified'][idx]}.\")\n",
    "            newfiles.append(pathfiles[idx])\n",
    "        else:\n",
    "            print(\"\")\n",
    "            print(f\"New data file found: {file_info_df['Filename'][idx]}, last update {file_info_df['Last Modified'][idx]}.\")\n",
    "            newfiles.append(pathfiles[idx])\n",
    "\n",
    "if len(newfiles) == 0:\n",
    "    \n",
    "    print(\"No new or changed data files found in live sources folder.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    pass\n",
    "\n",
    "dropped_files = []\n",
    "\n",
    "for r in prev_files[\"Filename\"]:\n",
    "    \n",
    "    if r in file_info_df[\"Filename\"].values:\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(f\"File {r} has been archived or removed from live sources folder.\")\n",
    "        dropped_files.append(os.path.join( ls_path, r))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "file_info_df.to_csv(file_list, index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d0f35",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Business Unit Check\n",
    "\n",
    "- import existing Business Unit and raw file list\n",
    "- enusure there is a 1 to 1 relationship\n",
    "- throw error if not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa05ff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [ \"Business Unit\", \"File\"]\n",
    "\n",
    "bu_x_file = pd.DataFrame(columns = cols ) \n",
    "\n",
    "for p, f  in zip(pathfiles,files_sour):\n",
    "\n",
    "    if s_format == \"x\":\n",
    "\n",
    "        try:\n",
    "            data = pd.read_excel(p, sheet_name = 'Portfolio_Monitoring', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "        except:\n",
    "            data = pd.read_excel(p, sheet_name = 'Ptf_Monitoring_GROSS_Reins', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "\n",
    "    elif s_format == \"j\":\n",
    "\n",
    "        data = pd.read_json(p, orient=\"table\")\n",
    "\n",
    "    elif s_format == \"p\":\n",
    "\n",
    "        data = pd.read_parquet(p, engine = \"pyarrow\")\n",
    "\n",
    "    bus = data[\"Business Unit\"].unique()\n",
    "    \n",
    "    for i in bus: bu_x_file = bu_x_file.append({\"Business Unit\": i,\"File\": f} , ignore_index=True)\n",
    "\n",
    "        \n",
    "#This is a list of BU's that are included in the raw data files, and the file name(s) they are contained in.\n",
    "bu_filelist = bu_x_file.sort_values(by='Business Unit').style.hide_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2e143",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "bucount =  bu_x_file.groupby(\"Business Unit\").count().sort_values(by='File', ascending=False)\n",
    "\n",
    "filecount =  bu_x_file.groupby(\"File\").count().sort_values(by='File')\n",
    "\n",
    "err_check = 1\n",
    "\n",
    "if len(bucount[bucount[\"File\"] > 1]) > 0: \n",
    "    warnings.warn(\"\\n \\nThere is a Business Unit with data in multiple source files.\\nReview the file counts and filenames to resolve the issue.\\nThis is a critical control.\")\n",
    "    err_check = 0\n",
    "else:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    1 / err_check\n",
    "\n",
    "except ZeroDivisionError:\n",
    "    print(\"Execution halted due to multiple source file per one or more Business Units.\")\n",
    "    1 / err_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d1354",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"This is a count of how many BUs appear in each file. It is fine to submit multiple BUs in one source.\")\n",
    "filecount\n",
    "\n",
    "\n",
    "print(\"\\nThis is a count of how many files each BU appears in. In all cases the count values should be one (1). The process as desgined, should have a single submission and source for each BU.\")\n",
    "bucount\n",
    "\n",
    "print(\"This is a list of BU's that are included in the raw data files, and the file name(s) they are contained in.\")   \n",
    "\n",
    "bu_filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6569ad3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "- Read each json file into temp DataFrame \"data\".\n",
    "- Prep actions - Strip (trim) leading and trailing spaces values in string columns, remove rows without business units.\n",
    "- Append each json output to into 1 DataFrame \"df\".\n",
    "- Process Cleanup, the indices restart with each append, reset and drop the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f77756",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "#empty dict to append each file's DataFrame to\n",
    "\n",
    "for f in pathfiles:\n",
    "\n",
    "    if s_format == \"x\":\n",
    "\n",
    "        try:\n",
    "            data = pd.read_excel(f, sheet_name = 'Portfolio_Monitoring', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "        except:\n",
    "            data = pd.read_excel(f, sheet_name = 'Ptf_Monitoring_GROSS_Reins', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "\n",
    "    elif s_format == \"j\":\n",
    "\n",
    "        data = pd.read_json(f, orient=\"table\")\n",
    "\n",
    "    elif s_format == \"p\":\n",
    "\n",
    "        data = pd.read_parquet(f, engine = \"pyarrow\")\n",
    "\n",
    "    df_dict.update({f: data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4464eb3c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Raw file Summaries\n",
    "\n",
    "#### First 5 rows of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a8c80",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - First 5 Samples:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.head())\n",
    "\n",
    "else:\n",
    "    print(\"Default Report 2 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff60256",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Summary Statisitics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed0cc9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - Statistical summary:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.describe())\n",
    "else:\n",
    "    print(\"Default Report 3 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbaebe7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Null Value Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f765d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - Null values in the dataset:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.isnull().sum())\n",
    "else:\n",
    "    print(\"Default Report 4 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf9d56",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Datatypes by field for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d3aed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - Datatypes:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.dtypes)\n",
    "else:\n",
    "    print(\"Default Report 5 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4737d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Add the datafile name as a column in each source DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb96781",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for p, f in zip(pathfiles, files_sour):\n",
    "    df_dict[p].insert(0, \"Submission File\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57f5af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### append into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df40ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for k, v in df_dict.items(): df = df.append(v)\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada86dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Create Temporary csv Output\n",
    "- Create a direct output of the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6347eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### This prep step helps parquet columns read columns with nulls or mixed dtypes. This is a good check that the data is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75948c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# add any float columns that give a mixed type error below\n",
    "v_flo = ['Open Claims %', 'Contribution Margin % on Earned Revenues net of Taxes - BU View', \n",
    "         'Contribution Margin % on Earned Revenues net of Taxes - HQ View', 'Loss Ratio', 'Commission Ratio',\n",
    "         'Expense Ratio', 'comsub', 'expsub','% of IBNR on (OCR + IBNR)', 'Contribution Margin % on Fixed Costs - BU View',\n",
    "        'Contribution Margin % on Fixed Costs - HQ View', 'Units of Risk (Written)', 'Written Revenues', \n",
    "         'Number of Policies (Earned)','Units of Risk (Earned)', 'Earned Revenues','Upfront Cash Payments',\n",
    "         'Number of Open Claims','Number of Persons Involved in Claims (Paid + OCR + IBNR)','Frequency (Earned)',\n",
    "         'Units of Risk (Written)' ,'Severity','Risk Premium','Unnamed: 66']\n",
    "\n",
    "\n",
    "# add any integer columns that give a mixed type error below\n",
    "v_int = ['Number of Products per Row',  'Number of B-Partners per Row']\n",
    "\n",
    "#v_obj = ['Units of Risk (Written)']\n",
    "\n",
    "\n",
    "# add any string columns that give a mixed type error below\n",
    "v_str = [ 'Distribution Type','Distribution Channel','Sub LOB', 'Business Partner ID Number', 'Product ID Number', 'Standard Product' ,'Notes']\n",
    "\n",
    "\n",
    "# add any datetime columns that give a mixed type error below\n",
    "#v_dat\n",
    "\n",
    "for i in v_flo:\n",
    "    df[i] = df[i].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df[i] = df[i].fillna(0)\n",
    "    df[i] = df[i].astype( 'float' )\n",
    "for i in v_int:\n",
    "    df[i] = df[i].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df[i] = df[i].fillna(0)\n",
    "    df[i] = df[i].astype('int64')\n",
    "\n",
    "for i in v_str:\n",
    "    df[i] = df[i].astype('str')\n",
    "    for i in v_str: df[i] = df[i].astype('str'  )\n",
    "\n",
    "#for i in v_dat: v[i] = v[i].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdfd2be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    prep_df = df\n",
    "    %store prep_df\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if run_control > 0:\n",
    "    \n",
    "    prepfile = f'{str(percode)}prep.csv'\n",
    "    prepparq = f'{str(percode)}prep.parquet'\n",
    "    prephistfile = f'{str(percode)}_{inst_datetime}prep.csv'\n",
    "\n",
    "    try:\n",
    "        os.rename(os.path.join(str(rt_path), prepfile), os.path.join(str(rt_path), \"logs/raw_data_file_history/\" , prephistfile))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    except FileExistsError:\n",
    "        os.remove(os.path.join(str(rt_path), \"logs/raw_data_file_history/\" , prephistfile))\n",
    "        os.rename(os.path.join(str(rt_path), prepfile), os.path.join(str(rt_path), \"logs/raw_data_file_history/\" , prephistfile))\n",
    "\n",
    "    df.to_csv(os.path.join(str(rt_path), prepfile), ',', index=False , encoding='utf-8-sig')\n",
    "    df.to_parquet(os.path.join(str(rt_path), prepparq),engine = \"pyarrow\")\n",
    "    \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696af83",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## This is considered \"pre-pipleine\" data prep. The next step is to read the resulting csv into the acutal data pipeline.\n",
    "\n",
    "## If you are not sure what is being loaded, this step provides a safe way to create a reviewable output file while keeping the production data files in tact.\n",
    "\n",
    "#### You can skip this step to speed up the procoess with run_contol == 0, not reccomended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237d975",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    dfp = (df.pivot_table(index=( \"Country\", \"Submission File\"), columns=(\"Reporting Date From\",\"Reporting Date To\" ), values=\"Earned Revenues net of Taxes\").fillna(0).astype(int))\n",
    "    dfp\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111a43b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Store the DataFrame for other noteboks to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc18ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986124d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.97433,
   "end_time": "2021-06-15T15:57:48.864741",
   "environment_variables": {},
   "exception": true,
   "input_path": "pipeline/prep.ipynb",
   "output_path": "//hecate/Insurance_US/Product Development/Product Management/Global PPM/Reporting/Data Collection/Pipeline Reporting Scripts/GPMpipeline/instances/2021.Q1\\pipeline\\prep06152021115711--live CHdistChan.ipynb",
   "parameters": {
    "commit_message": "Live run, add CH dist Channels",
    "inst_datetime": "06152021115711",
    "percode": "2021.Q1",
    "run_control": 1,
    "source_type": "p"
   },
   "start_time": "2021-06-15T15:57:14.890411",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}