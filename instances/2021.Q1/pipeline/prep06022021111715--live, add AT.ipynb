{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63edaaf",
   "metadata": {
    "papermill": {
     "duration": 0.389078,
     "end_time": "2021-06-02T15:17:33.790796",
     "exception": false,
     "start_time": "2021-06-02T15:17:33.401718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"../images/AzPTravel_PPM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e30ff",
   "metadata": {
    "papermill": {
     "duration": 0.391138,
     "end_time": "2021-06-02T15:17:34.547689",
     "exception": false,
     "start_time": "2021-06-02T15:17:34.156551",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Source File Consolidation\n",
    "\n",
    "#### This script fetches the current data collection submission files, consolidates them in to a combined csv file, and outputs that to the network.\n",
    "- All source files stored in the path are read, summarized, and then consolidated into a single dataframe named 'df'.\n",
    "- This process assumes that the multiple sources are in no way duplicates of each other.\n",
    "- df is then output in the data collection subfolder of the \"Production\" folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9259ca1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### User Variables\n",
    "- These are overwritten if inherited from run_control.ipynb.\n",
    "- Feel Free to reset them for a manual run if you like\n",
    "- Do not save without percode = \"-f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58060fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "commit_message = \"Development and testing.\"\n",
    "# Give a brief reason for the run.\n",
    "\n",
    "run_control = 1\n",
    "#run_type = 0 - Lite run with no reporting, not recommended.\n",
    "#run_type = 1 - Lite run with normal reporting, default setting.\n",
    "#run_type = 2 - Heavy run with full reporting, available for audits and troubleshooting.\n",
    "#run_type = 5 - A default setting. Indicates the script is being run by an outside process without an inherited value\n",
    "\n",
    "percode = \"2021.Q1\"\n",
    "# Data Collection Code, this controls file paths and output names\n",
    "# \"-f\" is the value indicating a bad inheritance from run with arg\n",
    "\n",
    "s_format = \"p\"\n",
    "# denotes the source data format x == Excel; j == json, p == parquet\n",
    "\n",
    "#----------\n",
    "# do not edit - this either inherits the full instance timestamp from the papermill book or captures the run time of this script.\n",
    "from datetime import datetime\n",
    "inst_datetime = datetime.now().strftime(\"%m%d%Y%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c98bc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_control = 1\n",
    "percode = \"2021.Q1\"\n",
    "commit_message = \"Live run, added AT, bad file name, rerun.\"\n",
    "inst_datetime = \"06022021111715\"\n",
    "source_type = \"p\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031c9fd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Notebook display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e009f16c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8736b63",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94c1af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Packages used\n",
    "\n",
    "import os # System commands\n",
    "import sys # System commands\n",
    "\n",
    "from datetime import datetime # datetime options\n",
    "import warnings # custom warnigns options\n",
    "\n",
    "import getpass # parquet file read/write\n",
    "import json # json file read/write\n",
    "\n",
    "import matplotlib.pyplot as plt #Plots and Graphs\n",
    "import openpyxl # Excel operations\n",
    "import numpy as np\n",
    "import pandas as pd #DataFrame and math\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6357343",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Default Variables, these govern logic, do not edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc320d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_dc = \"20XX.QX\"\n",
    "default_rc = 0 #extra lite mode\n",
    "dummy_perc = \"33Q3\" # bad inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235457f0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Script determining run context ie, manual, run_control.ipynb, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144536b5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "if run_control == 5:\n",
    "    run_control = default_rc \n",
    "else:\n",
    "    run_control = run_control\n",
    "\n",
    "try:\n",
    "    if sys.argv[1] == \"-f\":\n",
    "        percode = percode\n",
    "    else:\n",
    "        percode = sys.argv[1]\n",
    "\n",
    "except IndexError:\n",
    "    percode = default_dc\n",
    "except NameError:\n",
    "    percode = default_dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c74ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Make paths for the source folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4a421",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rt_path = f'//hecate/Insurance_US/Product Development/Product Management/Global PPM/Reporting/Data Collection/Production/{str(percode)}'\n",
    "ls_path = os.path.join( rt_path, 'live_sources')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7a3f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Get a list of only source files in the path that start with \"us_dat\".\n",
    "#### Logic determines the source file types.\n",
    "\n",
    "#### User instructions:\n",
    "- Make sure that you have 1 file per source in this folder.\n",
    "    -  For instance, do not have two files for Portugal. If there is an update, archive the old one.\n",
    "- Do not overwrite files in the archive.\n",
    "    - Rename newly archived files, no strict convention, we keep track of these by the modified date.\n",
    "- It is ok to have multiple sources in one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c1d47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = os.listdir(ls_path)\n",
    "files = [files.lower() for files in files]\n",
    "\n",
    "if s_format == \"x\":\n",
    "    files_sour = [f for f in files if f[-3:] in ('lsx' , 'lsm' ,'xls')]\n",
    "\n",
    "elif s_format == \"j\":\n",
    "    files_sour = [f for f in files if f[:6] == 'us_dat'  and  f[-5:]  == '.json']\n",
    "\n",
    "elif s_format == \"p\":\n",
    "    files_sour = [f for f in files if f[:6] == 'us_dat' and  f[-8:]  == '.parquet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9fbed7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Create a list 'pathfiles' that has every source file with the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdd362",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pathfiles = []\n",
    "\n",
    "for f in files_sour:\n",
    "    makepathsfiles = os.path.join(str(ls_path), str(f))\n",
    "    pathfiles.append(makepathsfiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb0cc9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### File Check\n",
    "\n",
    "- create file information Data.Frame \"file_info_df\" with name, size, and modified date of each source file\n",
    "- import previous source file list for comparison\n",
    "- print all new files, store as a list \"newfiles\"\n",
    "- print all dropped files, store as a list \"dropped_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822de89",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(\"Current Source Files\")\n",
    "cols = [\"Filename\", \"Size\", \"Last Modified\" ]\n",
    "\n",
    "file_info_df = pd.DataFrame(columns = cols)\n",
    "for f, p in zip(files_sour, pathfiles):\n",
    "    name = f\n",
    "    size = os.path.getsize(p) # size in bytes\n",
    "    moddate = os.path.getctime(p)# time of last metadata change;\n",
    "    moddate =  datetime.fromtimestamp(moddate).strftime('%Y-%m-%d %H:%M:%S')# format change;\n",
    "    new_row = pd.DataFrame([[name, size, moddate]], columns = cols)\n",
    "    file_info_df = file_info_df.append(new_row, ignore_index=True)\n",
    "\n",
    "#print(file_info_df)\n",
    "\n",
    "file_list = os.path.join( rt_path, 'Current_Source_Files.csv')\n",
    "prev_files = pd.read_csv(file_list)\n",
    "\n",
    "\n",
    "fl_comp = file_info_df[\"Filename\"] + file_info_df[\"Size\"].astype(str) + file_info_df[\"Last Modified\"].astype(str)\n",
    "\n",
    "prev_comp = prev_files[\"Filename\"] + prev_files[\"Size\"].astype(str) + prev_files[\"Last Modified\"].astype(str)\n",
    "\n",
    "newfiles = []\n",
    "\n",
    "for idx, r in enumerate(fl_comp):\n",
    "    \n",
    "    if r in prev_comp.values:\n",
    "    \n",
    "        pass\n",
    "     \n",
    "    else:\n",
    "        if file_info_df['Filename'][idx] in prev_files[\"Filename\"].values:\n",
    "            print(\"\")\n",
    "            print(f\"Changed data file found: {file_info_df['Filename'][idx]}, last update {file_info_df['Last Modified'][idx]}.\")\n",
    "            newfiles.append(pathfiles[idx])\n",
    "        else:\n",
    "            print(\"\")\n",
    "            print(f\"New data file found: {file_info_df['Filename'][idx]}, last update {file_info_df['Last Modified'][idx]}.\")\n",
    "            newfiles.append(pathfiles[idx])\n",
    "\n",
    "if len(newfiles) == 0:\n",
    "    \n",
    "    print(\"No new or changed data files found in live sources folder.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    pass\n",
    "\n",
    "dropped_files = []\n",
    "\n",
    "for r in prev_files[\"Filename\"]:\n",
    "    \n",
    "    if r in file_info_df[\"Filename\"].values:\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(f\"File {r} has been archived or removed from live sources folder.\")\n",
    "        dropped_files.append(os.path.join( ls_path, r))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "file_info_df.to_csv(file_list, index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a735366",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Business Unit Check\n",
    "\n",
    "- import existing Business Unit and raw file list\n",
    "- enusure there is a 1 to 1 relationship\n",
    "- throw error if not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86eb42",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [ \"Business Unit\", \"File\"]\n",
    "\n",
    "bu_x_file = pd.DataFrame(columns = cols ) \n",
    "\n",
    "for p, f  in zip(pathfiles,files_sour):\n",
    "\n",
    "    if s_format == \"x\":\n",
    "\n",
    "        try:\n",
    "            data = pd.read_excel(p, sheet_name = 'Portfolio_Monitoring', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "        except:\n",
    "            data = pd.read_excel(p, sheet_name = 'Ptf_Monitoring_GROSS_Reins', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "\n",
    "    elif s_format == \"j\":\n",
    "\n",
    "        data = pd.read_json(p, orient=\"table\")\n",
    "\n",
    "    elif s_format == \"p\":\n",
    "\n",
    "        data = pd.read_parquet(p, engine = \"pyarrow\")\n",
    "\n",
    "    bus = data[\"Business Unit\"].unique()\n",
    "    \n",
    "    for i in bus: bu_x_file = bu_x_file.append({\"Business Unit\": i,\"File\": f} , ignore_index=True)\n",
    "\n",
    "        \n",
    "#This is a list of BU's that are included in the raw data files, and the file name(s) they are contained in.\n",
    "bu_filelist = bu_x_file.sort_values(by='Business Unit').style.hide_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed51d3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "bucount =  bu_x_file.groupby(\"Business Unit\").count().sort_values(by='File', ascending=False)\n",
    "\n",
    "filecount =  bu_x_file.groupby(\"File\").count().sort_values(by='File')\n",
    "\n",
    "err_check = 1\n",
    "\n",
    "if len(bucount[bucount[\"File\"] > 1]) > 0: \n",
    "    warnings.warn(\"\\n \\nThere is a Business Unit with data in multiple source files.\\nReview the file counts and filenames to resolve the issue.\\nThis is a critical control.\")\n",
    "    err_check = 0\n",
    "else:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    1 / err_check\n",
    "\n",
    "except ZeroDivisionError:\n",
    "    print(\"Execution halted due to multiple source file per one or more Business Units.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d88b3b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"This is a count of how many BUs appear in each file. It is fine to submit multiple BUs in one source.\")\n",
    "filecount\n",
    "\n",
    "\n",
    "print(\"\\nThis is a count of how many files each BU appears in. In all cases the count values should be one (1). The process as desgined, should have a single submission and source for each BU.\")\n",
    "bucount\n",
    "\n",
    "print(\"This is a list of BU's that are included in the raw data files, and the file name(s) they are contained in.\")   \n",
    "\n",
    "bu_filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100757fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "- Read each json file into temp DataFrame \"data\".\n",
    "- Prep actions - Strip (trim) leading and trailing spaces values in string columns, remove rows without business units.\n",
    "- Append each json output to into 1 DataFrame \"df\".\n",
    "- Process Cleanup, the indices restart with each append, reset and drop the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d819f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "#empty dict to append each file's DataFrame to\n",
    "\n",
    "for f in pathfiles:\n",
    "\n",
    "    if s_format == \"x\":\n",
    "\n",
    "        try:\n",
    "            data = pd.read_excel(f, sheet_name = 'Portfolio_Monitoring', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "        except:\n",
    "            data = pd.read_excel(f, sheet_name = 'Ptf_Monitoring_GROSS_Reins', na_values = [0], header=3, converters={ 'Business Partner Name': str, 'Type of Business': str, 'Type of Account': str, 'Distribution Type': str, 'LOB': str, 'Distribution Channel': str,\n",
    "            'Sub LOB': str,'Business Partner ID Number': str,  'Product Name': str, 'Product ID Number': str,  'Product Family': str,  'Standard Product': str, 'Total Expenses': float,    })\n",
    "\n",
    "\n",
    "    elif s_format == \"j\":\n",
    "\n",
    "        data = pd.read_json(f, orient=\"table\")\n",
    "\n",
    "    elif s_format == \"p\":\n",
    "\n",
    "        data = pd.read_parquet(f, engine = \"pyarrow\")\n",
    "\n",
    "    df_dict.update({f: data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff3c3d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Raw file Summaries\n",
    "\n",
    "#### First 5 rows of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ec2fd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - First 5 Samples:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.head())\n",
    "\n",
    "else:\n",
    "    print(\"Default Report 2 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1397121",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Summary Statisitics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6efed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - Statistical summary:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.describe())\n",
    "else:\n",
    "    print(\"Default Report 3 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc03fa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Null Value Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13274d27",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - Null values in the dataset:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.isnull().sum())\n",
    "else:\n",
    "    print(\"Default Report 4 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c6e63",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Datatypes by field for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c153395",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    for k, v in df_dict.items():\n",
    "        print(f\"{k} - Datatypes:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(v.dtypes)\n",
    "else:\n",
    "    print(\"Default Report 5 Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73e764",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Add the datafile name as a column in each source DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50eccd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for p, f in zip(pathfiles, files_sour):\n",
    "    df_dict[p].insert(0, \"Submission File\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78454b25",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### append into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1edf80",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for k, v in df_dict.items(): df = df.append(v)\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f400f96",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Create Temporary csv Output\n",
    "- Create a direct output of the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18adf0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### This prep step helps parquet columns read columns with nulls or mixed dtypes. This is a good check that the data is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcda2e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# add any float columns that give a mixed type error below\n",
    "v_flo = ['Open Claims %', 'Contribution Margin % on Earned Revenues net of Taxes - BU View', \n",
    "         'Contribution Margin % on Earned Revenues net of Taxes - HQ View', 'Loss Ratio', 'Commission Ratio',\n",
    "         'Expense Ratio', 'comsub', 'expsub','% of IBNR on (OCR + IBNR)', 'Contribution Margin % on Fixed Costs - BU View',\n",
    "        'Contribution Margin % on Fixed Costs - HQ View', 'Units of Risk (Written)', 'Written Revenues', \n",
    "         'Number of Policies (Earned)','Units of Risk (Earned)', 'Earned Revenues','Upfront Cash Payments',\n",
    "         'Number of Open Claims','Number of Persons Involved in Claims (Paid + OCR + IBNR)','Frequency (Earned)',\n",
    "         'Units of Risk (Written)' ,'Severity','Risk Premium','Unnamed: 66']\n",
    "\n",
    "\n",
    "# add any integer columns that give a mixed type error below\n",
    "v_int = ['Number of Products per Row',  'Number of B-Partners per Row']\n",
    "\n",
    "#v_obj = ['Units of Risk (Written)']\n",
    "\n",
    "\n",
    "# add any string columns that give a mixed type error below\n",
    "v_str = [ 'Distribution Type','Distribution Channel','Sub LOB', 'Business Partner ID Number', 'Product ID Number', 'Standard Product' ,'Notes']\n",
    "\n",
    "\n",
    "# add any datetime columns that give a mixed type error below\n",
    "#v_dat\n",
    "\n",
    "for i in v_flo:\n",
    "    df[i] = df[i].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df[i] = df[i].fillna(0)\n",
    "    df[i] = df[i].astype( 'float' )\n",
    "for i in v_int:\n",
    "    df[i] = df[i].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df[i] = df[i].fillna(0)\n",
    "    df[i] = df[i].astype('int64')\n",
    "\n",
    "for i in v_str:\n",
    "    df[i] = df[i].astype('str')\n",
    "    for i in v_str: df[i] = df[i].astype('str'  )\n",
    "\n",
    "#for i in v_dat: v[i] = v[i].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5cd690",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    prep_df = df\n",
    "    %store prep_df\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if run_control > 0:\n",
    "    \n",
    "    prepfile = f'{str(percode)}prep.csv'\n",
    "    prepparq = f'{str(percode)}prep.parquet'\n",
    "    prephistfile = f'{str(percode)}_{inst_datetime}prep.csv'\n",
    "\n",
    "    try:\n",
    "        os.rename(os.path.join(str(rt_path), prepfile), os.path.join(str(rt_path), \"logs/raw_data_file_history/\" , prephistfile))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    except FileExistsError:\n",
    "        os.remove(os.path.join(str(rt_path), \"logs/raw_data_file_history/\" , prephistfile))\n",
    "        os.rename(os.path.join(str(rt_path), prepfile), os.path.join(str(rt_path), \"logs/raw_data_file_history/\" , prephistfile))\n",
    "\n",
    "    df.to_csv(os.path.join(str(rt_path), prepfile), ',', index=False , encoding='utf-8-sig')\n",
    "    df.to_parquet(os.path.join(str(rt_path), prepparq),engine = \"pyarrow\")\n",
    "    \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e2969",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## This is considered \"pre-pipleine\" data prep. The next step is to read the resulting csv into the acutal data pipeline.\n",
    "\n",
    "## If you are not sure what is being loaded, this step provides a safe way to create a reviewable output file while keeping the production data files in tact.\n",
    "\n",
    "#### You can skip this step to speed up the procoess with run_contol == 0, not reccomended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8751e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_control > 0:\n",
    "\n",
    "    dfp = (df.pivot_table(index=( \"Country\", \"Submission File\"), columns=(\"Reporting Date From\",\"Reporting Date To\" ), values=\"Earned Revenues net of Taxes\").fillna(0).astype(int))\n",
    "    dfp\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969997c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Store the DataFrame for other noteboks to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051d1e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867799c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.006133,
   "end_time": "2021-06-02T15:17:35.355763",
   "environment_variables": {},
   "exception": null,
   "input_path": "pipeline/prep.ipynb",
   "output_path": "//hecate/Insurance_US/Product Development/Product Management/Global PPM/Reporting/Data Collection/Pipeline Reporting Scripts/GPMpipeline/instances/2021.Q1\\pipeline\\prep06022021111715--live, add AT.ipynb",
   "parameters": {
    "commit_message": "Live run, added AT, bad file name, rerun.",
    "inst_datetime": "06022021111715",
    "percode": "2021.Q1",
    "run_control": 1,
    "source_type": "p"
   },
   "start_time": "2021-06-02T15:17:16.349630",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}